{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from keras import models\n",
    "from keras import callbacks\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import InputLayer\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Input\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import concatenate\n",
    "from keras_contrib.layers import CRF\n",
    "\n",
    "from importlib import reload\n",
    "\n",
    "from minuet import loader, encoder, datastream, minuet\n",
    "from minuet import preprocessing as p\n",
    "from minuet import minuet\n",
    "\n",
    "TRAIN_PATH = './data/pos/train.txt'\n",
    "DEVEL_PATH = './data/pos/dev.txt'\n",
    "GLOVE_PATH = './embeddings/wglove.6B.300d.bin'\n",
    "\n",
    "MAX_SENT_LEN = 10\n",
    "MAX_WORD_LEN = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_t, Y_t = loader.load_dataset(TRAIN_PATH)\n",
    "X_v, Y_v = loader.load_dataset(DEVEL_PATH)\n",
    "\n",
    "pre_word = p.assemble(\n",
    "    p.lower,\n",
    "    p.replace_numbers\n",
    ")\n",
    "\n",
    "pre_char = p.assemble(\n",
    "    p.replace_numbers\n",
    ")\n",
    "\n",
    "Vw = loader.get_vocabulary(X_t, pre_word)\n",
    "char2index = loader.get_characters_vocabulary(X_t, pre_char)\n",
    "\n",
    "word2index, E = loader.load_embeddings(GLOVE_PATH, Vw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating indices, encoding samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using padded samples\n",
      "['SOCCER', '-', 'JAPAN', 'GET', 'LUCKY', 'WIN', ',', 'CHINA', 'IN', 'SURPRISE', 'DEFEAT', '.']\n",
      "['NN', ':', 'NNP', 'VB', 'NNP', 'NNP', ',', 'NNP', 'IN', 'DT', 'NN', '.']\n",
      "[[19]\n",
      " [ 7]\n",
      " [20]\n",
      " [35]\n",
      " [20]\n",
      " [20]\n",
      " [ 5]\n",
      " [20]\n",
      " [13]\n",
      " [10]]\n"
     ]
    }
   ],
   "source": [
    "reload(encoder)\n",
    "\n",
    "# word embedding indices\n",
    "E_sent_t = encoder.sentence_to_index(X_t, word2index, pre_word, MAX_SENT_LEN)\n",
    "E_sent_v = encoder.sentence_to_index(X_v, word2index, pre_word, MAX_SENT_LEN)\n",
    "\n",
    "# char embedding indices\n",
    "E_word_t = encoder.sentence_to_characters(X_t, char2index, MAX_WORD_LEN, MAX_SENT_LEN, f=pre_char)\n",
    "E_word_v = encoder.sentence_to_characters(X_v, char2index, MAX_WORD_LEN, MAX_SENT_LEN, f=pre_char)\n",
    "\n",
    "# labels indices\n",
    "Y_train, label_encoder = encoder.encode_labels(Y_t, MAX_SENT_LEN)\n",
    "Y_valid, _ = encoder.encode_labels(Y_v, MAX_SENT_LEN, label_encoder)\n",
    "\n",
    "if MAX_SENT_LEN:\n",
    "    print('Using padded samples')\n",
    "    Y_train = np.expand_dims(Y_train, -1)\n",
    "    Y_valid = np.expand_dims(Y_valid, -1)\n",
    "else:\n",
    "    raise NotImplementedError(':)')\n",
    "    \n",
    "print(X_v[0])\n",
    "print(Y_v[0])\n",
    "print(Y_valid[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "char_input (InputLayer)         (None, None, None)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sent_input (InputLayer)         (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "char_embedding (Embedding)      (None, None, None, 2 1925        char_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "word_embedding (Embedding)      (None, None, 300)    4793100     sent_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "char_BiLSTM (TimeDistributed)   (None, None, 50)     10200       char_embedding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_15 (Concatenate)    (None, None, 350)    0           word_embedding[0][0]             \n",
      "                                                                 char_BiLSTM[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "sent_BiLSTM (Bidirectional)     (None, None, 60)     91440       concatenate_15[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "softmax (Dense)                 (None, None, 45)     2745        sent_BiLSTM[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 4,899,410\n",
      "Trainable params: 106,310\n",
      "Non-trainable params: 4,793,100\n",
      "__________________________________________________________________________________________________\n",
      "(1, 10, 45)\n",
      "Train on 14041 samples, validate on 3453 samples\n",
      "Epoch 1/5\n",
      "14041/14041 [==============================] - 33s 2ms/step - loss: 1.5783 - sparse_categorical_accuracy: 0.5943 - val_loss: 0.7424 - val_sparse_categorical_accuracy: 0.8154\n",
      "Epoch 2/5\n",
      "14041/14041 [==============================] - 18s 1ms/step - loss: 0.7312 - sparse_categorical_accuracy: 0.8055 - val_loss: 0.5276 - val_sparse_categorical_accuracy: 0.8635\n",
      "Epoch 3/5\n",
      "14041/14041 [==============================] - 18s 1ms/step - loss: 0.5666 - sparse_categorical_accuracy: 0.8446 - val_loss: 0.4524 - val_sparse_categorical_accuracy: 0.8832\n",
      "Epoch 4/5\n",
      "14041/14041 [==============================] - 18s 1ms/step - loss: 0.4892 - sparse_categorical_accuracy: 0.8648 - val_loss: 0.4113 - val_sparse_categorical_accuracy: 0.8927\n",
      "Epoch 5/5\n",
      "14041/14041 [==============================] - 18s 1ms/step - loss: 0.4372 - sparse_categorical_accuracy: 0.8774 - val_loss: 0.3828 - val_sparse_categorical_accuracy: 0.8983\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd700f903c8>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CHAR_EMBEDDING = 25\n",
    "LSTM_CHAR_SIZE = 16\n",
    "LSTM_CHAR_DROP = 0.5\n",
    "\n",
    "def build_character_embedding():\n",
    "    # shape_in: (batch_size, sentence_words_maxlen, chars_maxlen [ids])\n",
    "    chars_input = Input(shape=(None, None), name='char_input')\n",
    "    char_embedding = Embedding(\n",
    "        len(char2index), CHAR_EMBEDDING, embeddings_initializer='glorot_uniform',\n",
    "        mask_zero=True, name='char_embedding'\n",
    "    )(chars_input)\n",
    "    #shape_out: (batch_size, sentence_maxlen, chars_maxlen, char_embedding_dim)\n",
    "\n",
    "    char_embedding = TimeDistributed(\n",
    "        Bidirectional(\n",
    "            LSTM(\n",
    "                CHAR_EMBEDDING, dropout=LSTM_CHAR_DROP,\n",
    "                recurrent_dropout=LSTM_CHAR_DROP, name='char_LSTM'\n",
    "            )\n",
    "        ),\n",
    "        name='char_BiLSTM'\n",
    "    )(char_embedding)\n",
    "    # shape_out: (batch_size, sentence_words_maxlen, charlstm_hidden_dim_size)\n",
    "    \n",
    "    return chars_input, char_embedding\n",
    "\n",
    "def build_word_embedding(E):\n",
    "    # shape_in: (batch_size, sentence_words_maxlen)\n",
    "    words_input = Input(shape=(None,), name='sent_input')\n",
    "    \n",
    "    word_embedding = Embedding(\n",
    "        E.shape[0], E.shape[1],\n",
    "        weights=[E], trainable=False,\n",
    "        mask_zero=True, name='word_embedding'\n",
    "    )(words_input)\n",
    "    # shape_out: (batch_size, sentence_words_maxlen, word_embedding_dim)\n",
    "    \n",
    "    return words_input, word_embedding\n",
    "\n",
    "def build_sentence_lstm(word_embedding, char_embedding, lstm_size, dropout_proba, bidirectional):\n",
    "    if char_embedding is None:\n",
    "        word_representations = word_embedding\n",
    "    elif word_embedding is None:\n",
    "        word_representations = char_embedding\n",
    "    else:\n",
    "        word_representations = concatenate([word_embedding, char_embedding], axis=-1)\n",
    "    \n",
    "    # shape_in: (batch_size, sentence_words_maxlen, [word_embedding_dim + char_embedding_dim] = d)\n",
    "    lstm = LSTM(\n",
    "        lstm_size, dropout=dropout_proba, recurrent_dropout=dropout_proba,\n",
    "        return_sequences=True, name='sent_LSTM'\n",
    "    )\n",
    "    \n",
    "    if bidirectional:\n",
    "        lstm = Bidirectional(lstm, name='sent_BiLSTM')\n",
    "    sentence_representations = lstm(word_representations)\n",
    "    # shape_in: (batch_size, sentence_words_maxlen, d, lstm_hidden_size*(1 or 2))\n",
    "        \n",
    "    return sentence_representations\n",
    "\n",
    "def build_softmax_output(sentence_representations, n_labels):\n",
    "    # shape_in: (batch_size, sentence_words_maxlen, lstm_hidden_size*(1 or 2))\n",
    "    # By default Dense only operates on the last layer\n",
    "    out = Dense(n_labels, activation='softmax',\n",
    "                name='softmax')(sentence_representations)\n",
    "    \n",
    "    return out, 'sparse_categorical_crossentropy', ['sparse_categorical_accuracy']\n",
    "    # shape_out: (batch_size, sentence_words_maxlen, n_classes)\n",
    "\n",
    "def build_crf_output(sentence_representations, n_labels):\n",
    "    crf = CRF(n_labels, sparse_target=True)\n",
    "    out = crf(sentence_representations)\n",
    "    \n",
    "    return out, crf.loss_function, [crf.accuracy]\n",
    "\n",
    "\n",
    "words_input, word_embedding = build_word_embedding(E)\n",
    "chars_input, char_embedding = build_character_embedding()\n",
    "sentence_vectors = build_sentence_lstm(word_embedding, char_embedding, 30, 0.5, bidirectional=True)\n",
    "out, loss, acc = build_softmax_output(sentence_vectors, len(label_encoder.classes_))\n",
    "# out, loss, acc = build_crf_output(sentence_vectors, len(label_encoder.classes_))\n",
    "\n",
    "model = models.Model(\n",
    "    inputs=[words_input, chars_input],\n",
    "    outputs=[out]\n",
    ")\n",
    "\n",
    "opt = optimizers.Adam(clipnorm=5.0)\n",
    "model.compile(opt, loss=loss, metrics=acc)\n",
    "model.summary()\n",
    "\n",
    "\n",
    "w = model.predict([E_sent_t[:1], E_word_t[:1]])\n",
    "print(w.shape)\n",
    "\n",
    "model.fit(\n",
    "    [E_sent_t, E_word_t], Y_train, batch_size=32,\n",
    "    epochs=5,\n",
    "    validation_data=([E_sent_v, E_word_v], Y_valid)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
